{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 1.4829 - accuracy: 0.6231 - val_loss: 0.7584 - val_accuracy: 0.8286\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.6049 - accuracy: 0.8464 - val_loss: 0.4550 - val_accuracy: 0.8852\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.4398 - accuracy: 0.8801 - val_loss: 0.3710 - val_accuracy: 0.9019\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.3767 - accuracy: 0.8952 - val_loss: 0.3322 - val_accuracy: 0.9082\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.3415 - accuracy: 0.9025 - val_loss: 0.3055 - val_accuracy: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.3175 - accuracy: 0.9086 - val_loss: 0.2880 - val_accuracy: 0.9182\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.2989 - accuracy: 0.9137 - val_loss: 0.2727 - val_accuracy: 0.9224\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2839 - accuracy: 0.9180 - val_loss: 0.2608 - val_accuracy: 0.9266\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.2714 - accuracy: 0.9217 - val_loss: 0.2505 - val_accuracy: 0.9298\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.2602 - accuracy: 0.9252 - val_loss: 0.2430 - val_accuracy: 0.9308\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.2501 - accuracy: 0.9285 - val_loss: 0.2341 - val_accuracy: 0.9335\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.2409 - accuracy: 0.9301 - val_loss: 0.2271 - val_accuracy: 0.9352\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.2325 - accuracy: 0.9334 - val_loss: 0.2227 - val_accuracy: 0.9367\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.2253 - accuracy: 0.9353 - val_loss: 0.2147 - val_accuracy: 0.9396\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.2181 - accuracy: 0.9375 - val_loss: 0.2082 - val_accuracy: 0.9411\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.2116 - accuracy: 0.9394 - val_loss: 0.2030 - val_accuracy: 0.9431\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.2055 - accuracy: 0.9414 - val_loss: 0.1981 - val_accuracy: 0.9445\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.1996 - accuracy: 0.9430 - val_loss: 0.1932 - val_accuracy: 0.9458\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.1941 - accuracy: 0.9432 - val_loss: 0.1894 - val_accuracy: 0.9467\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.1890 - accuracy: 0.9456 - val_loss: 0.1849 - val_accuracy: 0.9498\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "Test score: 0.18599770209044217\n",
      "Test accuracy: 0.9463000297546387\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline values for accuracy rate and validation using data sets with two hidden layers\n",
    "Accuracy on training - 94.56%\n",
    "Validation - 94.67%\n",
    "Accuracy on test - 94.63%\n",
    "\n",
    "\n",
    "Thoughts on changes that can/can't be made\n",
    "- I don't want to change NB_CLASSES as the goal is to get the system to identify the 10 distinct numbers. \n",
    "- I don't want to change RESHAPED either as that's the 1D vector for flattening MNIST images\n",
    "- Others I won't change - verbose, \n",
    "\n",
    "- I could adjust the VALIDATION_SPLIT - changing the amount of the dataset the system will train or validate on could possibly have negative effects on accuracy and validation. My prediction is that these values may be inversely related. A potential consequence of adjusting to less validation data would lead to noisy/less representative validation accuracy. \n",
    "    **edit - I learned that on smaller data sets it may be better to lower the validation amount but increase the epochs to allow more training time\n",
    "\n",
    "- I could adjust the OPTIMIZER from just using the default hyperparameters of SGD. One possible change is to pass more parameters through like for adjusting momentum and rate of learning. Another potential change is to use an alternative, common, optimizer; Adam (another optimizer) adjusts learning rate for each param to help handle gradient variations. A potential issue that may arise with changing the optimizer is that if it converges too quickly it might end up overfitting. \n",
    "\n",
    "- Lastly, I can adjust the NB_EPOCH. More epochs could likely lead to higher training and validation accuracy, however this can lead to overfitting or overly long training times. Too few epochs can cause underfitting, lower accuracy, but is much faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.3509 - accuracy: 0.8982 - val_loss: 0.1713 - val_accuracy: 0.9502\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.1390 - accuracy: 0.9592 - val_loss: 0.1297 - val_accuracy: 0.9617\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.0949 - accuracy: 0.9710 - val_loss: 0.1026 - val_accuracy: 0.9692\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.0700 - accuracy: 0.9789 - val_loss: 0.0950 - val_accuracy: 0.9707\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.0540 - accuracy: 0.9837 - val_loss: 0.0956 - val_accuracy: 0.9724\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.0423 - accuracy: 0.9864 - val_loss: 0.0949 - val_accuracy: 0.9726\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.0330 - accuracy: 0.9906 - val_loss: 0.0916 - val_accuracy: 0.9742\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.0959 - val_accuracy: 0.9752\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.0227 - accuracy: 0.9928 - val_loss: 0.0915 - val_accuracy: 0.9762\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.0191 - accuracy: 0.9943 - val_loss: 0.1096 - val_accuracy: 0.9713\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.0155 - accuracy: 0.9953 - val_loss: 0.1086 - val_accuracy: 0.9732\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.0114 - accuracy: 0.9967 - val_loss: 0.1077 - val_accuracy: 0.9735\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.0095 - accuracy: 0.9972 - val_loss: 0.1242 - val_accuracy: 0.9725\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.0141 - accuracy: 0.9952 - val_loss: 0.1139 - val_accuracy: 0.9747\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 0.1055 - val_accuracy: 0.9784\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.1120 - val_accuracy: 0.9776\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.1236 - val_accuracy: 0.9758\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.1571 - val_accuracy: 0.9712\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.1234 - val_accuracy: 0.9761\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.1275 - val_accuracy: 0.9765\n",
      "10000/10000 [==============================] - 1s 52us/step\n",
      "Test score: 0.1067015897124037\n",
      "Test accuracy: 0.9772999882698059\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD, Adam  # Added Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1671)\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10\n",
    "OPTIMIZER = Adam(learning_rate=0.001)  # <-- Changed from SGD to Adam (with learning rate param)\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE,\n",
    "                    validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New values from only changing the optimizer\n",
    "Accuracy on training - 99.89% (+5.33%)\n",
    "Validation - 97.65% (+2.98%)\n",
    "Accuracy on test - 97.73% (+3.1%)\n",
    "\n",
    "The large increase in the accuracy on training was more expected as Adam converges faster than SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 30000 samples, validate on 30000 samples\n",
      "Epoch 1/20\n",
      "30000/30000 [==============================] - 4s 119us/step - loss: 1.7111 - accuracy: 0.5826 - val_loss: 1.1262 - val_accuracy: 0.7801\n",
      "Epoch 2/20\n",
      "30000/30000 [==============================] - 3s 101us/step - loss: 0.8083 - accuracy: 0.8254 - val_loss: 0.6225 - val_accuracy: 0.8500\n",
      "Epoch 3/20\n",
      "30000/30000 [==============================] - 3s 97us/step - loss: 0.5404 - accuracy: 0.8658 - val_loss: 0.4860 - val_accuracy: 0.8707\n",
      "Epoch 4/20\n",
      "30000/30000 [==============================] - 3s 90us/step - loss: 0.4495 - accuracy: 0.8807 - val_loss: 0.4260 - val_accuracy: 0.8828\n",
      "Epoch 5/20\n",
      "30000/30000 [==============================] - 3s 89us/step - loss: 0.4027 - accuracy: 0.8904 - val_loss: 0.3903 - val_accuracy: 0.8895\n",
      "Epoch 6/20\n",
      "30000/30000 [==============================] - 3s 87us/step - loss: 0.3734 - accuracy: 0.8966 - val_loss: 0.3673 - val_accuracy: 0.8942\n",
      "Epoch 7/20\n",
      "30000/30000 [==============================] - 3s 92us/step - loss: 0.3515 - accuracy: 0.9031 - val_loss: 0.3502 - val_accuracy: 0.8974\n",
      "Epoch 8/20\n",
      "30000/30000 [==============================] - 3s 90us/step - loss: 0.3344 - accuracy: 0.9061 - val_loss: 0.3349 - val_accuracy: 0.9023\n",
      "Epoch 9/20\n",
      "30000/30000 [==============================] - 3s 98us/step - loss: 0.3206 - accuracy: 0.9109 - val_loss: 0.3247 - val_accuracy: 0.9059\n",
      "Epoch 10/20\n",
      "30000/30000 [==============================] - 3s 101us/step - loss: 0.3084 - accuracy: 0.9133 - val_loss: 0.3127 - val_accuracy: 0.9097\n",
      "Epoch 11/20\n",
      "30000/30000 [==============================] - 3s 96us/step - loss: 0.2978 - accuracy: 0.9170 - val_loss: 0.3036 - val_accuracy: 0.9120\n",
      "Epoch 12/20\n",
      "30000/30000 [==============================] - 3s 92us/step - loss: 0.2885 - accuracy: 0.9186 - val_loss: 0.2973 - val_accuracy: 0.9128\n",
      "Epoch 13/20\n",
      "30000/30000 [==============================] - 3s 86us/step - loss: 0.2795 - accuracy: 0.9221 - val_loss: 0.2883 - val_accuracy: 0.9148\n",
      "Epoch 14/20\n",
      "30000/30000 [==============================] - 3s 92us/step - loss: 0.2718 - accuracy: 0.9237 - val_loss: 0.2823 - val_accuracy: 0.9185\n",
      "Epoch 15/20\n",
      "30000/30000 [==============================] - 3s 93us/step - loss: 0.2640 - accuracy: 0.9273 - val_loss: 0.2759 - val_accuracy: 0.9194\n",
      "Epoch 16/20\n",
      "30000/30000 [==============================] - 3s 92us/step - loss: 0.2572 - accuracy: 0.9276 - val_loss: 0.2712 - val_accuracy: 0.9205\n",
      "Epoch 17/20\n",
      "30000/30000 [==============================] - 3s 93us/step - loss: 0.2505 - accuracy: 0.9304 - val_loss: 0.2651 - val_accuracy: 0.9222\n",
      "Epoch 18/20\n",
      "30000/30000 [==============================] - 3s 94us/step - loss: 0.2445 - accuracy: 0.9311 - val_loss: 0.2588 - val_accuracy: 0.9238\n",
      "Epoch 19/20\n",
      "30000/30000 [==============================] - 3s 92us/step - loss: 0.2385 - accuracy: 0.9335 - val_loss: 0.2540 - val_accuracy: 0.9263\n",
      "Epoch 20/20\n",
      "30000/30000 [==============================] - 3s 92us/step - loss: 0.2334 - accuracy: 0.9348 - val_loss: 0.2508 - val_accuracy: 0.9265\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "Test score: 0.23559223500043153\n",
      "Test accuracy: 0.9334999918937683\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1671)\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10\n",
    "OPTIMIZER = SGD()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.5  # changed 0.2 to 0.5 (I want to see bigger changes with possible negative effect)\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE,\n",
    "                    validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New values from adjusting only the validation split\n",
    "Accuracy on training - 93.48 (-1.08%)\n",
    "Validation - 92.65 (-2.02%)\n",
    "Accuracy on test - 93.34 (-1.29%)\n",
    "\n",
    "The overall reduction was expected, considering there's less training data and time. However, I was surprised by the small change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/60\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 1.4881 - accuracy: 0.6275 - val_loss: 0.7539 - val_accuracy: 0.8407\n",
      "Epoch 2/60\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.5994 - accuracy: 0.8511 - val_loss: 0.4538 - val_accuracy: 0.8841\n",
      "Epoch 3/60\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.4392 - accuracy: 0.8807 - val_loss: 0.3739 - val_accuracy: 0.8972\n",
      "Epoch 4/60\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.3789 - accuracy: 0.8947 - val_loss: 0.3362 - val_accuracy: 0.9061\n",
      "Epoch 5/60\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.3447 - accuracy: 0.9029 - val_loss: 0.3105 - val_accuracy: 0.9111\n",
      "Epoch 6/60\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.3209 - accuracy: 0.9093 - val_loss: 0.2931 - val_accuracy: 0.9142\n",
      "Epoch 7/60\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.3024 - accuracy: 0.9143 - val_loss: 0.2781 - val_accuracy: 0.9198\n",
      "Epoch 8/60\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2873 - accuracy: 0.9187 - val_loss: 0.2657 - val_accuracy: 0.9227\n",
      "Epoch 9/60\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.2749 - accuracy: 0.9215 - val_loss: 0.2560 - val_accuracy: 0.9263\n",
      "Epoch 10/60\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.2638 - accuracy: 0.9250 - val_loss: 0.2488 - val_accuracy: 0.9284\n",
      "Epoch 11/60\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.2538 - accuracy: 0.9278 - val_loss: 0.2396 - val_accuracy: 0.9298\n",
      "Epoch 12/60\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.2446 - accuracy: 0.9295 - val_loss: 0.2329 - val_accuracy: 0.9334\n",
      "Epoch 13/60\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.2364 - accuracy: 0.9320 - val_loss: 0.2281 - val_accuracy: 0.9352\n",
      "Epoch 14/60\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.2291 - accuracy: 0.9338 - val_loss: 0.2204 - val_accuracy: 0.9382\n",
      "Epoch 15/60\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.2219 - accuracy: 0.9362 - val_loss: 0.2139 - val_accuracy: 0.9391\n",
      "Epoch 16/60\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2153 - accuracy: 0.9382 - val_loss: 0.2088 - val_accuracy: 0.9417\n",
      "Epoch 17/60\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.2091 - accuracy: 0.9398 - val_loss: 0.2039 - val_accuracy: 0.9434\n",
      "Epoch 18/60\n",
      "48000/48000 [==============================] - 3s 73us/step - loss: 0.2032 - accuracy: 0.9413 - val_loss: 0.1995 - val_accuracy: 0.9432\n",
      "Epoch 19/60\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1978 - accuracy: 0.9429 - val_loss: 0.1956 - val_accuracy: 0.9460\n",
      "Epoch 20/60\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.1926 - accuracy: 0.9445 - val_loss: 0.1911 - val_accuracy: 0.9463\n",
      "Epoch 21/60\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.1876 - accuracy: 0.9466 - val_loss: 0.1878 - val_accuracy: 0.9482\n",
      "Epoch 22/60\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.1828 - accuracy: 0.9476 - val_loss: 0.1851 - val_accuracy: 0.9477\n",
      "Epoch 23/60\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.1786 - accuracy: 0.9485 - val_loss: 0.1815 - val_accuracy: 0.9494\n",
      "Epoch 24/60\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.1745 - accuracy: 0.9498 - val_loss: 0.1784 - val_accuracy: 0.9504\n",
      "Epoch 25/60\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.1702 - accuracy: 0.9511 - val_loss: 0.1747 - val_accuracy: 0.9521\n",
      "Epoch 26/60\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.1663 - accuracy: 0.9524 - val_loss: 0.1722 - val_accuracy: 0.9531\n",
      "Epoch 27/60\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.1626 - accuracy: 0.9533 - val_loss: 0.1697 - val_accuracy: 0.9534\n",
      "Epoch 28/60\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.1589 - accuracy: 0.9545 - val_loss: 0.1667 - val_accuracy: 0.9541\n",
      "Epoch 29/60\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.1555 - accuracy: 0.9551 - val_loss: 0.1633 - val_accuracy: 0.9557\n",
      "Epoch 30/60\n",
      "48000/48000 [==============================] - 5s 99us/step - loss: 0.1522 - accuracy: 0.9564 - val_loss: 0.1614 - val_accuracy: 0.9559\n",
      "Epoch 31/60\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.1490 - accuracy: 0.9573 - val_loss: 0.1586 - val_accuracy: 0.9570\n",
      "Epoch 32/60\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.1459 - accuracy: 0.9581 - val_loss: 0.1577 - val_accuracy: 0.9579\n",
      "Epoch 33/60\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.1430 - accuracy: 0.9590 - val_loss: 0.1551 - val_accuracy: 0.9587\n",
      "Epoch 34/60\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.1402 - accuracy: 0.9600 - val_loss: 0.1537 - val_accuracy: 0.9594\n",
      "Epoch 35/60\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.1375 - accuracy: 0.9604 - val_loss: 0.1504 - val_accuracy: 0.9598\n",
      "Epoch 36/60\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.1347 - accuracy: 0.9618 - val_loss: 0.1487 - val_accuracy: 0.9604\n",
      "Epoch 37/60\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.1322 - accuracy: 0.9620 - val_loss: 0.1469 - val_accuracy: 0.9603\n",
      "Epoch 38/60\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1296 - accuracy: 0.9628 - val_loss: 0.1458 - val_accuracy: 0.9602\n",
      "Epoch 39/60\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.1272 - accuracy: 0.9640 - val_loss: 0.1428 - val_accuracy: 0.9617\n",
      "Epoch 40/60\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.1247 - accuracy: 0.9645 - val_loss: 0.1418 - val_accuracy: 0.9619\n",
      "Epoch 41/60\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.1224 - accuracy: 0.9651 - val_loss: 0.1405 - val_accuracy: 0.9621\n",
      "Epoch 42/60\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.1202 - accuracy: 0.9658 - val_loss: 0.1383 - val_accuracy: 0.9619\n",
      "Epoch 43/60\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.1181 - accuracy: 0.9665 - val_loss: 0.1377 - val_accuracy: 0.9635\n",
      "Epoch 44/60\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.1157 - accuracy: 0.9672 - val_loss: 0.1358 - val_accuracy: 0.9638\n",
      "Epoch 45/60\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.1139 - accuracy: 0.9682 - val_loss: 0.1340 - val_accuracy: 0.9636\n",
      "Epoch 46/60\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.1118 - accuracy: 0.9681 - val_loss: 0.1325 - val_accuracy: 0.9642\n",
      "Epoch 47/60\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.1098 - accuracy: 0.9689 - val_loss: 0.1317 - val_accuracy: 0.9643\n",
      "Epoch 48/60\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.1081 - accuracy: 0.9693 - val_loss: 0.1314 - val_accuracy: 0.9643\n",
      "Epoch 49/60\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.1062 - accuracy: 0.9695 - val_loss: 0.1282 - val_accuracy: 0.9653\n",
      "Epoch 50/60\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.1045 - accuracy: 0.9705 - val_loss: 0.1280 - val_accuracy: 0.9655\n",
      "Epoch 51/60\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.1027 - accuracy: 0.9709 - val_loss: 0.1272 - val_accuracy: 0.9654\n",
      "Epoch 52/60\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.1010 - accuracy: 0.9712 - val_loss: 0.1256 - val_accuracy: 0.9658\n",
      "Epoch 53/60\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0992 - accuracy: 0.9724 - val_loss: 0.1253 - val_accuracy: 0.9656\n",
      "Epoch 54/60\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0978 - accuracy: 0.9728 - val_loss: 0.1243 - val_accuracy: 0.9654\n",
      "Epoch 55/60\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0962 - accuracy: 0.9730 - val_loss: 0.1226 - val_accuracy: 0.9660\n",
      "Epoch 56/60\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0947 - accuracy: 0.9734 - val_loss: 0.1228 - val_accuracy: 0.9666\n",
      "Epoch 57/60\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.0930 - accuracy: 0.9738 - val_loss: 0.1204 - val_accuracy: 0.9668\n",
      "Epoch 58/60\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0917 - accuracy: 0.9746 - val_loss: 0.1194 - val_accuracy: 0.9662\n",
      "Epoch 59/60\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0903 - accuracy: 0.9748 - val_loss: 0.1188 - val_accuracy: 0.9665\n",
      "Epoch 60/60\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0888 - accuracy: 0.9756 - val_loss: 0.1182 - val_accuracy: 0.9663\n",
      "10000/10000 [==============================] - 0s 37us/step\n",
      "Test score: 0.1111958737745881\n",
      "Test accuracy: 0.9668999910354614\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1671)\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 60  # Changed from 20 to 60\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10\n",
    "OPTIMIZER = SGD()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NB_EPOCH,  # 40 epochs\n",
    "                    verbose=VERBOSE,\n",
    "                    validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New values from only changing the epoch value\n",
    "Accuracy on training - 97.56% (+3.00%)\n",
    "Validation - 96.63% (+1.96%)\n",
    "Accuracy on test - 96.69% (+2.06%)\n",
    "\n",
    "Increasing the training time can improve the accuracy if it was underfitting previously. Something to watch for is overfitting - validation accuracy begins plateuing or declines after some number of epochs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
